# ðŸ§  Primer Proyecto de Inteligencia Artificial â€” ITCR (Sede San Carlos)

Un sistema distribuido compuesto por microservicios de Machine Learning y un coordinador LLM local. El objetivo es combinar modelos predictivos tradicionales con capacidades conversacionales para que los usuarios exploren y consulten resultados mediante lenguaje natural.

## Destacado
- Arquitectura basada en microservicios (API REST para modelos)
- Coordinador inteligente que integra un LLM local (Ollama / LLaMA) para diÃ¡logo y explicaciÃ³n
- Modelos especializados para regresiÃ³n, clasificaciÃ³n, recomendaciÃ³n y **series de tiempo**
- Nuevo modelo Prophet para predicciones temporales de Bitcoin
- Interfaz conversacional para consultas en lenguaje natural

## Modelos incluidos

| Modelo                           | Tipo                    | PropÃ³sito                                         | Estado    |
|:--------------------------------:|:-----------------------:|:--------------------------------------------------|:----------|
| `prophet_bitcoin_v2_*.pkl`      | **Series de Tiempo**   | **PredicciÃ³n temporal del precio del Bitcoin**   | âœ… Activo |
| `knn_movie_recommendation_model.pkl` | RecomendaciÃ³n      | Sugerencia de pelÃ­culas segÃºn preferencias       | âœ… Activo |
| `random_forest_properties_*.pkl` | RegresiÃ³n             | PredicciÃ³n del precio de propiedades             | âœ… Activo |
| `ACV_decision_tree_model.pkl`    | ClasificaciÃ³n          | DetecciÃ³n de riesgo de accidente cerebrovascular | âœ… Activo |
| `bitcoin_random_forest_*.pkl`    | RegresiÃ³n              | PredicciÃ³n Bitcoin (modelo anterior)             | ðŸ“¦ Legacy |

> **Nuevo**: El modelo de Bitcoin ahora usa **Prophet** para anÃ¡lisis de series temporales, permitiendo predicciones mÃ¡s precisas con tendencias estacionales y intervalos de confianza.

## Requisitos

- Python 3.9+ (recomendado)
- pip, virtualenv (o venv)
- Node.js + npm (para el frontend)
- macOS: Homebrew (para instalar Ollama si se usa)

### Dependencias principales nuevas:
- **Prophet**: Para modelos de series temporales
- **joblib**: Para carga optimizada de modelos ML
- **FastAPI**: APIs REST modernas y eficientes

Instala dependencias Python:

```bash
python3 -m venv venv
source venv/bin/activate   # macOS / Linux (zsh compatible)
pip install -r requirements.txt
```

## Variables de entorno (recomendadas)

Exporta estas variables en tu shell o crea un archivo `.env` (no incluir en Git):

```bash
export ENV=development
export AZURE_FACE_KEY="tu_api_key"
export AZURE_FACE_ENDPOINT="https://<endpoint>.cognitiveservices.azure.com/"
export LLM_HOST="http://localhost:8001"
export API_BASE_URL="http://localhost:8080"
```

## ConfiguraciÃ³n del LLM (opcional: Ollama)

Si quieres correr un LLM local con Ollama (opcional):

```bash
# macOS (Homebrew)
brew install ollama
ollama pull llama3:3b

# Ejecutar el modelo para pruebas
ollama run llama3:3b
```

Dependiendo de tu arquitectura y recursos, puedes elegir otro modelo o servicio. El coordinador LLM del repo asume que hay un endpoint local en `LLM_HOST`.

## EjecuciÃ³n â€” servicios individuales

### 1) Backend (API de modelos) â€” **Recomendado**

```bash
source venv/bin/activate
./run_api.sh  # Script optimizado que usa uvicorn correctamente
```

O manualmente:
```bash
source venv/bin/activate
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

### 2) Coordinador LLM

```bash
source venv/bin/activate
python llm/coordinator.py
```

### 3) Frontend (interfaz)

```bash
cd frontend
npm install
npm run dev
```

## Nuevas funcionalidades

### ðŸ”® Predicciones temporales de Bitcoin
El nuevo modelo Prophet permite consultas como:
- "Â¿CuÃ¡l serÃ¡ el precio de Bitcoin maÃ±ana?"
- "Predice Bitcoin para la prÃ³xima semana"
- "Â¿QuÃ© precio tendrÃ¡ Bitcoin el 1 de enero de 2025?"

### ðŸ¤– Coordinador inteligente mejorado
- ExtracciÃ³n automÃ¡tica de fechas y parÃ¡metros
- Respuestas contextuales segÃºn el tipo de modelo
- Manejo de errores y respaldos automÃ¡ticos

## Script unificado (run_all.sh)

Hay un script de conveniencia `run_all.sh` que arranca los componentes en segundo plano. Antes de usarlo, asegÃºrate de que los comandos existentes (uvicorn, ollama, python) funcionen desde tu shell.

Contenido de ejemplo (ya incluido en el repo):

```bash
#!/bin/bash
source venv/bin/activate
echo "ðŸš€ Iniciando API de Machine Learning..."
uvicorn api.main:app --port 8000 &

echo "ðŸ§  Iniciando LLM (Ollama) si estÃ¡ instalado..."
ollama serve &

echo "ðŸ”— Iniciando coordinador LLM..."
python3 llm/coordinator.py &

echo "ðŸ’» Iniciando frontend..."
cd frontend || cd interface
npm run dev
```

Haz el script ejecutable y ejecÃºtalo:

```bash
chmod +x run_all.sh
./run_all.sh
```

## Estructura del proyecto (resumen)

- `api/` â€” microservicio(s) y endpoints para los modelos ML
- `llm/` â€” coordinador/puente entre los modelos y el LLM
- `frontend/` o `interface/` â€” interfaz React (UI)
- `models/` â€” modelos preentrenados (.pkl u otros)
- `data/` â€” datos raw/processed/examples
- `notebooks/` â€” notebooks exploratorios
- `tests/` â€” pruebas unitarias y de integraciÃ³n

## SoluciÃ³n de problemas comÃºn

- "ModuleNotFoundError" â€” activa el venv y reinstala dependencias: `pip install -r requirements.txt`.
- Problemas con Ollama â€” verificar versiÃ³n y que el servicio estÃ© corriendo: `ollama ps` / `ollama logs`.
- Frontend no arranca â€” revisa `node` y `npm` instalados, luego `npm install` y `npm run dev`.

## Buenas prÃ¡cticas

- MantÃ©n credenciales fuera del repositorio (.env en .gitignore).
- Versiona modelos con nombres que incluyan versiÃ³n y fecha cuando sean reentrenados.


## Contacto y crÃ©ditos

- Autor/es: Sebastian Matey, Liz Salazar, Roosevelt PÃ©rez â€” Instituto TecnolÃ³gico de Costa Rica (Sede San Carlos)
- Repo: ProyectoIA



