# ðŸ§  Primer Proyecto de Inteligencia Artificial â€” ITCR (Sede San Carlos)

Un sistema distribuido compuesto por microservicios de Machine Learning y un coordinador LLM local. El objetivo es combinar modelos predictivos tradicionales con capacidades conversacionales para que los usuarios exploren y consulten resultados mediante lenguaje natural.

Destacado
- Arquitectura basada en microservicios (API REST para modelos).
- Un coordinador que integra un LLM local (p. ej. Ollama / LLaMA) para diÃ¡logo y explicaciÃ³n.
- Conjuntos de modelos para regresiÃ³n, clasificaciÃ³n y recomendaciÃ³n listos para desplegar.

## Modelos incluidos

| Archivo                  | Tipo          | PropÃ³sito                                         |
|-------------------------:|:-------------:|:--------------------------------------------------|
| `bitcoin_model.pkl`      | RegresiÃ³n     | PredicciÃ³n del precio del Bitcoin                  |
| `movies_model.pkl`       | RecomendaciÃ³n | Sugerencia de pelÃ­culas segÃºn preferencias         |
| `house_model.pkl`        | RegresiÃ³n     | PredicciÃ³n del precio de viviendas                 |
| `stroke_model.pkl`       | ClasificaciÃ³n | DetecciÃ³n de riesgo de accidente cerebrovascular   |
| `flight_delay_model.pkl` | RegresiÃ³n     | PredicciÃ³n de retrasos en vuelos                   |

> Nota: Los modelos listados suelen estar en la carpeta `models/` (ver estructura del proyecto).

## Requisitos

- Python 3.10+ (recomendado)
- pip, virtualenv (o venv)
- Node.js + npm (para el frontend)
- macOS: Homebrew (para instalar Ollama si se usa)

Instala dependencias Python:

```bash
python3 -m venv venv
source venv/bin/activate   # macOS / Linux (zsh compatible)
pip install -r requirements.txt
```

## Variables de entorno (recomendadas)

Exporta estas variables en tu shell o crea un archivo `.env` (no incluir en Git):

```bash
export ENV=development
export AZURE_FACE_KEY="tu_api_key"
export AZURE_FACE_ENDPOINT="https://<endpoint>.cognitiveservices.azure.com/"
export LLM_HOST="http://localhost:8001"
export API_BASE_URL="http://localhost:8080"
```

## ConfiguraciÃ³n del LLM (opcional: Ollama)

Si quieres correr un LLM local con Ollama (opcional):

```bash
# macOS (Homebrew)
brew install ollama
ollama pull llama3:3b

# Ejecutar el modelo para pruebas
ollama run llama3:3b
```

Dependiendo de tu arquitectura y recursos, puedes elegir otro modelo o servicio. El coordinador LLM del repo asume que hay un endpoint local en `LLM_HOST`.

## EjecuciÃ³n â€” servicios individuales

1) Backend (API de modelos)

```bash
source venv/bin/activate
uvicorn api.main:app --reload --port 8000
```

2) Coordinador LLM

```bash
python3 llm/coordinator.py
```

3) Frontend (interfaz)

```bash
cd frontend || cd interface  # revisar el nombre correcto de la carpeta en tu repo
npm install
npm run dev
```

Si la carpeta del frontend se llama `interface` en tu repositorio original, usa esa en lugar de `frontend`.

## Script unificado (run_all.sh)

Hay un script de conveniencia `run_all.sh` que arranca los componentes en segundo plano. Antes de usarlo, asegÃºrate de que los comandos existentes (uvicorn, ollama, python) funcionen desde tu shell.

Contenido de ejemplo (ya incluido en el repo):

```bash
#!/bin/bash
source venv/bin/activate
echo "ðŸš€ Iniciando API de Machine Learning..."
uvicorn api.main:app --port 8000 &

echo "ðŸ§  Iniciando LLM (Ollama) si estÃ¡ instalado..."
ollama serve &

echo "ðŸ”— Iniciando coordinador LLM..."
python3 llm/coordinator.py &

echo "ðŸ’» Iniciando frontend..."
cd frontend || cd interface
npm run dev
```

Haz el script ejecutable y ejecÃºtalo:

```bash
chmod +x run_all.sh
./run_all.sh
```

## Estructura del proyecto (resumen)

- `api/` â€” microservicio(s) y endpoints para los modelos ML
- `llm/` â€” coordinador/puente entre los modelos y el LLM
- `frontend/` o `interface/` â€” interfaz React (UI)
- `models/` â€” modelos preentrenados (.pkl u otros)
- `data/` â€” datos raw/processed/examples
- `notebooks/` â€” notebooks exploratorios
- `tests/` â€” pruebas unitarias y de integraciÃ³n

## SoluciÃ³n de problemas comÃºn

- "ModuleNotFoundError" â€” activa el venv y reinstala dependencias: `pip install -r requirements.txt`.
- Problemas con Ollama â€” verificar versiÃ³n y que el servicio estÃ© corriendo: `ollama ps` / `ollama logs`.
- Frontend no arranca â€” revisa `node` y `npm` instalados, luego `npm install` y `npm run dev`.

## Buenas prÃ¡cticas

- MantÃ©n credenciales fuera del repositorio (.env en .gitignore).
- Versiona modelos con nombres que incluyan versiÃ³n y fecha cuando sean reentrenados.


## Contacto y crÃ©ditos

- Autor/es: Sebastian Matey, Liz Salazar, Roosevelt PÃ©rez â€” Instituto TecnolÃ³gico de Costa Rica (Sede San Carlos)
- Repo: ProyectoIA



