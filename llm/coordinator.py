from langchain_community.llms import Ollama
import requests
import json

llm = Ollama(model="llama3")

# Diccionario de modelos disponibles
ENDPOINTS = {
    "bitcoin": "http://localhost:8000/bitcoin/models/bitcoin/predict",
}

def interpretar_y_ejecutar(query: str):
    """
    Coordinador principal que decide qu√© modelo usar y ejecuta la consulta
    """
    # Paso 1: el LLM decide qu√© modelo usar
    decision_prompt = f"""
    Eres un coordinador de modelos de IA. Analiza la siguiente consulta y decide qu√© modelo usar.

    Consulta: "{query}"

    Modelos disponibles:
    - bitcoin: Para predicciones de precios de Bitcoin, criptomonedas, an√°lisis financiero
    - wine: Para clasificaci√≥n de vinos (no disponible a√∫n)
    - churn: Para predicci√≥n de abandono de clientes (no disponible a√∫n)
    - movies: Para recomendaciones de pel√≠culas (no disponible a√∫n)
    - emotions: Para an√°lisis de emociones (no disponible a√∫n)

    Responde SOLO con el nombre del modelo m√°s apropiado (bitcoin, wine, churn, movies, emotions).
    Si no hay un modelo apropiado, responde "ninguno".
    """
    
    decision = llm.invoke(decision_prompt)
    modelo = decision.strip().lower()

    # Paso 2: llama al modelo correspondiente
    if modelo in ENDPOINTS:
        try:
            data = {"query": query}
            response = requests.post(ENDPOINTS[modelo], json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
            else:
                return f"Error al consultar el modelo {modelo}: {response.status_code} - {response.text}"
                
        except requests.exceptions.RequestException as e:
            return f"Error de conexi√≥n con el modelo {modelo}: {str(e)}"
        except Exception as e:
            return f"Error inesperado al consultar {modelo}: {str(e)}"
    else:
        if modelo == "ninguno":
            return "Lo siento, no tengo un modelo espec√≠fico para responder a esa consulta. Actualmente solo puedo ayudarte con predicciones de precios de Bitcoin."
        else:
            return f"El modelo '{modelo}' no est√° disponible actualmente. Solo tengo disponible el modelo de Bitcoin para predicciones de precios de criptomonedas."

    # Paso 3: interpreta el resultado con el LLM
    interpretation_prompt = f"""
    Un modelo de {modelo} devolvi√≥ este resultado para la consulta "{query}":

    Resultado: {json.dumps(result, indent=2)}

    Tu tarea es interpretar este resultado y explic√°rselo al usuario de forma natural, clara y √∫til.

    Instrucciones:
    1. Si es una predicci√≥n de Bitcoin, incluye el precio predicho, la tendencia y el nivel de confianza
    2. Explica qu√© significa el resultado en t√©rminos simples
    3. Menciona cualquier limitaci√≥n o consideraci√≥n importante
    4. S√© conciso pero informativo
    5. Usa emojis apropiados para hacer la respuesta m√°s amigable

    Respuesta:
    """

    try:
        explicacion = llm.invoke(interpretation_prompt)
        return explicacion
    except Exception as e:
        # Si falla la interpretaci√≥n, devolver el resultado raw de forma m√°s amigable
        if modelo == "bitcoin" and "prediction" in result:
            prediction = result.get("prediction", 0)
            confidence = result.get("confidence", 0)
            return f"üí∞ Predicci√≥n de Bitcoin: ${prediction:,.2f} USD (Confianza: {confidence:.1f}%)"
        else:
            return f"Resultado del modelo {modelo}: {result}"
