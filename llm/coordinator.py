from langchain_community.llms import Ollama
import requests
import json

llm = Ollama(model="llama3")

# Configuraci贸n de modelos disponibles
MODELS_CONFIG = {
    "bitcoin": {
        "endpoint": "http://localhost:8000/bitcoin/models/bitcoin/predict",
        "description": "Para predicciones de precios de Bitcoin, criptomonedas, an谩lisis financiero",
        "available": True,
        "response_type": "prediction"
    },
    # TODO: Agregar configuraci贸n para otros modelos cuando est茅n implementados
    "wine": {
        "endpoint": "http://localhost:8000/wine/classify",  # TODO: Implementar endpoint
        "description": "Para clasificaci贸n de vinos basada en caracter铆sticas qu铆micas",
        "available": False,
        "response_type": "classification"
    },
    "churn": {
        "endpoint": "http://localhost:8000/churn/predict",  # TODO: Implementar endpoint
        "description": "Para predicci贸n de abandono de clientes",
        "available": False,
        "response_type": "prediction"
    },
    "movies": {
        "endpoint": "http://localhost:8000/movies/recommend",  # TODO: Implementar endpoint
        "description": "Para recomendaciones de pel铆culas personalizadas",
        "available": False,
        "response_type": "recommendation"
    },
    "emotions": {
        "endpoint": "http://localhost:8000/emotions/analyze",  # TODO: Implementar endpoint
        "description": "Para an谩lisis de emociones en texto",
        "available": False,
        "response_type": "classification"
    }
}

def get_available_models():
    """Retorna lista de modelos disponibles"""
    return {name: config for name, config in MODELS_CONFIG.items() if config["available"]}

def interpretar_y_ejecutar(query: str):
    """
    Coordinador principal que decide qu茅 modelo usar y ejecuta la consulta
    """
    # Paso 1: el LLM decide qu茅 modelo usar
    available_models = get_available_models()
    
    # Construir la descripci贸n de modelos disponibles din谩micamente
    models_description = "\n".join([
        f"    - {name}: {config['description']}"
        for name, config in MODELS_CONFIG.items()
        if config["available"]
    ])
    
    # Agregar modelos no disponibles
    unavailable_models = "\n".join([
        f"    - {name}: {config['description']} (no disponible a煤n)"
        for name, config in MODELS_CONFIG.items()
        if not config["available"]
    ])
    
    decision_prompt = f"""
    Eres un coordinador de modelos de IA. Analiza la siguiente consulta y decide qu茅 modelo usar.

    Consulta: "{query}"

    Modelos disponibles:
{models_description}

    Modelos en desarrollo:
{unavailable_models}

    Responde SOLO con el nombre del modelo m谩s apropiado ({', '.join(MODELS_CONFIG.keys())}).
    Si no hay un modelo apropiado, responde "ninguno".
    """
    
    decision = llm.invoke(decision_prompt)
    modelo = decision.strip().lower()

    # Paso 2: verificar si el modelo est谩 disponible y hacer la consulta
    if modelo in MODELS_CONFIG:
        model_config = MODELS_CONFIG[modelo]
        
        if not model_config["available"]:
            return f"El modelo '{modelo}' est谩 en desarrollo y no est谩 disponible a煤n. Actualmente solo tengo disponible: {', '.join(get_available_models().keys())}"
        
        # Hacer la consulta al modelo
        try:
            data = {"query": query}
            response = requests.post(model_config["endpoint"], json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
            else:
                return f"Error al consultar el modelo {modelo}: {response.status_code} - {response.text}"
                
        except requests.exceptions.RequestException as e:
            return f"Error de conexi贸n con el modelo {modelo}: {str(e)}"
        except Exception as e:
            return f"Error inesperado al consultar {modelo}: {str(e)}"
    else:
        if modelo == "ninguno":
            available_list = ', '.join(get_available_models().keys())
            return f"Lo siento, no tengo un modelo espec铆fico para responder a esa consulta. Actualmente puedo ayudarte con: {available_list}"
        else:
            return f"El modelo '{modelo}' no existe. Modelos disponibles: {', '.join(get_available_models().keys())}"

    # Paso 3: interpreta el resultado con el LLM
    interpretation_prompt = f"""
    Un modelo de {modelo} (tipo: {model_config['response_type']}) devolvi贸 este resultado para la consulta "{query}":

    Resultado: {json.dumps(result, indent=2)}

    Tu tarea es interpretar este resultado y explic谩rselo al usuario de forma natural, clara y 煤til.

    Instrucciones espec铆ficas seg煤n el tipo de modelo:
    - Si es 'prediction' (predicci贸n): Incluye el valor predicho, tendencia y nivel de confianza
    - Si es 'classification' (clasificaci贸n): Explica la categor铆a predicha y probabilidad
    - Si es 'recommendation' (recomendaci贸n): Lista las recomendaciones principales y razones

    Instrucciones generales:
    1. Explica qu茅 significa el resultado en t茅rminos simples
    2. Menciona cualquier limitaci贸n o consideraci贸n importante
    3. S茅 conciso pero informativo
    4. Usa emojis apropiados para hacer la respuesta m谩s amigable

    Respuesta:
    """

    try:
        explicacion = llm.invoke(interpretation_prompt)
        return explicacion
    except Exception as e:
        # Si falla la interpretaci贸n, devolver el resultado de forma m谩s amigable
        return format_fallback_response(modelo, result, model_config['response_type'])

def format_fallback_response(modelo: str, result: dict, response_type: str):
    """
    Formatea una respuesta de respaldo cuando falla la interpretaci贸n del LLM
    """
    try:
        if response_type == "prediction":
            if modelo == "bitcoin" and "prediction" in result:
                prediction = result.get("prediction", 0)
                confidence = result.get("confidence", 0)
                return f" Predicci贸n de Bitcoin: ${prediction:,.2f} USD (Confianza: {confidence:.1f}%)"
            # TODO: Agregar formato para otros modelos de predicci贸n (churn, etc.)
            
        elif response_type == "classification":
            # TODO: Implementar formato para modelos de clasificaci贸n (wine, emotions)
            if "predicted_class" in result:
                predicted_class = result.get("predicted_class", "Desconocido")
                probability = result.get("probability", 0)
                return f" Clasificaci贸n: {predicted_class} (Probabilidad: {probability:.1f}%)"
                
        elif response_type == "recommendation":
            # TODO: Implementar formato para modelos de recomendaci贸n (movies)
            if "recommendations" in result:
                recs = result.get("recommendations", [])[:3]  # Top 3
                return f" Recomendaciones: {', '.join(recs)}"
        
        # Respuesta gen茅rica si no hay formato espec铆fico
        return f"Resultado del modelo {modelo}: {json.dumps(result, indent=2)}"
        
    except Exception:
        return f"Resultado del modelo {modelo}: {result}"
