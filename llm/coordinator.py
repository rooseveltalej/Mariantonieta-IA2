from langchain_community.llms import Ollama
import requests
import json
from .extract_params import (
    extract_bitcoin_parameters,
    extract_flights_parameters,
    extract_properties_parameters,
    extract_movies_parameters,
    extract_acv_parameters,
    extract_avocado_parameters
)
from .available_models import MODELS_CONFIG

llm = Ollama(model="llama3")


def get_available_models():
    """Retorna lista de modelos disponibles"""
    return {name: config for name, config in MODELS_CONFIG.items() if config["available"]}

def interpretar_y_ejecutar(query: str):
    """
    Coordinador principal que decide qu√© modelo usar y ejecuta la consulta
    """
    # Paso 1: el LLM decide qu√© modelo usar
    available_models = get_available_models()
    
    # Construir la descripci√≥n de modelos disponibles din√°micamente
    models_description = "\n".join([
        f"    - {name}: {config['description']}"
        for name, config in MODELS_CONFIG.items()
        if config["available"]
    ])
    
    # Agregar modelos no disponibles
    unavailable_models = "\n".join([
        f"    - {name}: {config['description']} (no disponible a√∫n)"
        for name, config in MODELS_CONFIG.items()
        if not config["available"]
    ])
    
    decision_prompt = f"""
    Eres un coordinador de modelos de IA. Analiza la siguiente consulta y decide qu√© modelo usar.

    Consulta: "{query}"

    Modelos disponibles:
    {models_description}

    Modelos en desarrollo:
    {unavailable_models}

    Responde SOLO con el nombre del modelo m√°s apropiado ({', '.join(MODELS_CONFIG.keys())}).
    Si no hay un modelo apropiado, responde "ninguno".
    """
    
    decision = llm.invoke(decision_prompt)
    modelo = decision.strip().lower()

    # Paso 2: verificar si el modelo est√° disponible y hacer la consulta
    if modelo in MODELS_CONFIG:
        model_config = MODELS_CONFIG[modelo]
        
        if not model_config["available"]:
            return f"El modelo '{modelo}' est√° en desarrollo y no est√° disponible a√∫n. Actualmente solo tengo disponible: {', '.join(get_available_models().keys())}"
        
        # Hacer la consulta al modelo
        try:
            data = {"query": query}
            
            # Extraer par√°metros espec√≠ficos seg√∫n el modelo
            if modelo == "bitcoin":
                bitcoin_params = extract_bitcoin_parameters(query, llm)
                if bitcoin_params:
                    data.update(bitcoin_params)
                    print(f"üéØ Par√°metros extra√≠dos para Bitcoin: {bitcoin_params}")
            
            elif modelo == "flights":
                flights_params = extract_flights_parameters(query, llm)
                if flights_params:
                    data.update(flights_params)
                    print(f"‚úàÔ∏è Par√°metros extra√≠dos para Vuelos: {flights_params}")
            
            elif modelo == "properties":
                properties_params = extract_properties_parameters(query, llm)
                if properties_params:
                    data.update(properties_params)
                    print(f"üè† Par√°metros extra√≠dos para Propiedades: {properties_params}")
            
            elif modelo == "movies":
                movies_params = extract_movies_parameters(query, llm)
                if movies_params:
                    data.update(movies_params)
                    print(f"üé¨ Par√°metros extra√≠dos para Pel√≠culas: {movies_params}")
                
                # Para pel√≠culas, podr√≠amos necesitar un endpoint diferente si es predicci√≥n de rating
                if "user_id" in data and "movie_id" in data:
                    model_config["endpoint"] = "http://localhost:8000/movies/models/movies/predict-rating"
            
            elif modelo == "acv":
                acv_params = extract_acv_parameters(query, llm)
                if acv_params:
                    data.update(acv_params)
                    print(f"üè• Par√°metros extra√≠dos para ACV: {acv_params}")
            
            elif modelo == "avocado":
                avocado_params = extract_avocado_parameters(query, llm)
                if avocado_params:
                    data.update(avocado_params)
                    print(f"ü•ë Par√°metros extra√≠dos para Aguacate: {avocado_params}")
            
            response = requests.post(model_config["endpoint"], json=data, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
            else:
                return f"Error al consultar el modelo {modelo}: {response.status_code} - {response.text}"
                
        except requests.exceptions.RequestException as e:
            return f"Error de conexi√≥n con el modelo {modelo}: {str(e)}"
        except Exception as e:
            return f"Error inesperado al consultar {modelo}: {str(e)}"
    else:
        if modelo == "ninguno":
            available_list = ', '.join(get_available_models().keys())
            return f"Lo siento, no tengo un modelo espec√≠fico para responder a esa consulta. Actualmente puedo ayudarte con: {available_list}"
        else:
            return f"El modelo '{modelo}' no existe. Modelos disponibles: {', '.join(get_available_models().keys())}"

    # Paso 3: interpreta el resultado con el LLM
    interpretation_prompt = f"""
    Un modelo de {modelo} (tipo: {model_config['response_type']}) devolvi√≥ este resultado para la consulta "{query}":

    Resultado: {json.dumps(result, indent=2)}

    Tu tarea es interpretar este resultado y explic√°rselo al usuario de forma natural, clara y √∫til.

        Instrucciones espec√≠ficas seg√∫n el tipo de modelo:
    - Si es 'time_series_prediction' (predicci√≥n temporal): Explica las tendencias, fechas espec√≠ficas, valores predichos y intervalos de confianza
    - Si es 'flight_prediction' (predicci√≥n de vuelos): Explica el retraso esperado, factores que influyen, nivel de confianza y recomendaciones
    - Si es 'prediction' (predicci√≥n): Incluye el valor predicho, tendencia y nivel de confianza
    - Si es 'classification' (clasificaci√≥n): Explica la categor√≠a predicha y probabilidad
    - Si es 'recommendation' (recomendaci√≥n): Lista las recomendaciones principales y razones
    - Si es 'medical_classification' (clasificaci√≥n m√©dica): Explica el nivel de riesgo, probabilidad, factores de riesgo identificados y recomendaciones m√©dicas

    Para predicciones de Bitcoin con Prophet:
    - Menciona las fechas espec√≠ficas y sus precios predichos
    - Explica la tendencia general (alcista, bajista, estable)
    - Incluye los intervalos de confianza si est√°n disponibles
    - Menciona limitaciones del modelo (predicciones son estimaciones)

    Instrucciones generales:
    1. Explica qu√© significa el resultado en t√©rminos simples
    2. Menciona cualquier limitaci√≥n o consideraci√≥n importante
    3. S√© conciso pero informativo
    4. Usa emojis apropiados para hacer la respuesta m√°s amigable

    Respuesta:
    """

    try:
        # Siempre usar el LLM para generar una respuesta conversacional completa
        explicacion = llm.invoke(interpretation_prompt)
        return explicacion
    except Exception as e:
        # Si falla la interpretaci√≥n, devolver el resultado de forma m√°s amigable
        return format_fallback_response(modelo, result, model_config['response_type'])

def format_fallback_response(modelo: str, result: dict, response_type: str):
    """
    Formatea una respuesta de respaldo cuando falla la interpretaci√≥n del LLM
    """
    try:
        if response_type == "prediction":
            if modelo == "bitcoin" and "prediction" in result:
                prediction = result.get("prediction", 0)
                confidence = result.get("confidence", 0)
                return f"üí∞ Predicci√≥n Bitcoin: ${prediction:,.2f} USD (Confianza: {confidence:.1f}%)"
                
        elif response_type == "prediction":
            if modelo == "properties" and "prediction" in result:
                prediction = result.get("prediction", 0)
                confidence = result.get("confidence", 0)
                return f"üè† Precio estimado de propiedad: ${prediction:,.2f} USD (Confianza: {confidence:.1f}%)"
            
            # TODO: Agregar formato para otros modelos de predicci√≥n (churn, etc.)
            
        elif response_type == "classification":
            # TODO: Implementar formato para modelos de clasificaci√≥n (wine, emotions)
            if "predicted_class" in result:
                predicted_class = result.get("predicted_class", "Desconocido")
                probability = result.get("probability", 0)
                return f"üéØ Clasificaci√≥n: {predicted_class} (Probabilidad: {probability:.1f}%)"
        
        elif response_type == "medical_classification":
            if modelo == "acv":
                prediction = result.get("prediction", 0)
                probability = result.get("probability", 0)
                risk_level = result.get("risk_level", "Desconocido")
                confidence = result.get("confidence", 0)
                
                if prediction == 1:
                    return f"‚ö†Ô∏è RIESGO ALTO de ACV: {probability:.1%} probabilidad - Nivel: {risk_level} (Confianza: {confidence:.1f}%)"
                else:
                    return f"‚úÖ RIESGO BAJO de ACV: {(1-probability):.1%} probabilidad - Nivel: {risk_level} (Confianza: {confidence:.1f}%)"
                
        elif response_type == "recommendation":
            if modelo == "movies":
                if "recommendations" in result:
                    recs = result.get("recommendations", [])[:3]  # Top 3
                    if recs:
                        movie_titles = [rec.get("title", "Pel√≠cula desconocida") for rec in recs]
                        return f"üé¨ Recomendaciones de pel√≠culas: {', '.join(movie_titles)}"
                
                elif "predicted_rating" in result:
                    rating = result.get("predicted_rating", 0)
                    confidence = result.get("confidence", 0)
                    movie_title = result.get("model_info", {}).get("movie_title", "Pel√≠cula")
                    return f"üé¨ Rating predicho para {movie_title}: {rating:.1f}/5.0 (Confianza: {confidence:.1f}%)"
        
        # Respuesta gen√©rica si no hay formato espec√≠fico
        return f"Resultado del modelo {modelo}: {json.dumps(result, indent=2)}"
        
    except Exception:
        return f"Resultado del modelo {modelo}: {result}"
